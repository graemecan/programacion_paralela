
<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="chrome=1" />

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />


<title>Modern architectures</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<!-- General and theme style sheets -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/css/reveal.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/css/theme/white.css" id="theme">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/lib/css/zenburn.css">

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
}

</script>

<!--[if lt IE 9]>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/lib/js/html5shiv.js"></script>
<![endif]-->

<!-- Loading the mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: false }
        }
    });
    </script>
    <!-- End of mathjax configuration -->

<!-- Get Font-awesome from cdn -->
<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css">-->


        <style type="text/css">
            .container{
                        display: flex;
                      }
            .col{
                      flex: 1;
                }
            .reveal section p {
                      display: inline-block;
                      font-size: 0.6em;
                      line-height: 1.2em;
                      vertical-align: top;
                      text-align: left;
            }
            .reveal section li {
                      font-size: 0.6em;
            }
            .reveal section td {
                      font-size: 0.6em;
            }
            .reveal section img {
                      border: none;
            }
        </style>

	</head>
	<body>
			<div class="reveal">
			<div class="slides">

                <section><h1>Modern Architectures</h1>
                </section>

				<section><h3>Memory hierarchy: von Neumann bottleneck</h3>
                <p>In a classic von Neumann architecture a CPU is connected to main memory through a bus.</p>
                <img src="modern_architectures_figs/3-1_von_neumann.png">
                </section>

                <section><h4>Memory hierarchy: von Neumann bottleneck</h4>
                <p>CPU compute speeds are now much higher than memory access speeds: <i>von Neumann bottleneck</i>.</p>
                <p>Example: 8 core CPU, 3 GHz clock frequency, we assume 16 double-precision flops per clock cycle.</p>
                <p>Peak compute performance $3$ GHz $\times 8 \times 16 = 384$ GFlops/s.</p>
                <p>Peak memory transfer rate of connected DRAM module is $51.2$ GB/s.</p>
                </section>

                <section><h4>Memory hierarchy: von Neumann bottleneck</h4>
                <p>Consider the dot product of two large vectors ($n = 2^{30}$):</p>
                <pre><code>double dotp = 0.0;
for (int i = 0; i &lt n; i++)
    dotp += u[i] * v[i];</code></pre>
                <p>Two operations in each iteration: $2 \cdot n = 2^{31}$ Flops. Memory transfer of $2^{31} \times 8$ B $= 16$ GB.</p>
                </section>

                <section><h4>Memory hierarchy: von Neumann bottleneck</h4>
                <p><ul><li>Computation: $t_{comp}$ = 2 GFlops/384 GFlops/s = 5.2 ms.</li>
                       <li>Data transfer: $t_{mem}$ = 16 GB/51.2 GB/s = 312.5 ms.</li></ul></p>
                <p>Thus, assuming overlap of computation and memory access, $t_{exec} \geq \text{max}(t_{comp},t_{mem}) = 312.5$ ms.</p>
                <p>The data transfer time dominates. Data is used once with no re-usage: this computation is <i>memory-bound</i>.</p>
                <p>Maximum acheivable performance is therefore $2^{31} \text{Flop}/312.5 \text{ms} = 6.4$ GFlop/s, 2% of peak performance.</p>
                </section>

                <section><h4>Memory hierarchy: von Neumann bottleneck</h4>
                <p>To avoid this bottleneck, CPUs now have "on-board" fast memory referred to as <b>cache</b>.</p>
                <img src="modern_architectures_figs/3-2_cache.png">
                </section>

                <section><h4>Memory hierarchy: von Neumann bottleneck</h4>
                <p>Typically there are $3$ levels, L1, L2 and L3, on a modern CPU. GPUs typically have $2$ levels.</p>
                <p>L1 is small and fast, L3 big and slow. Caches may be private for a single core or shared amongst several cores.</p>
                <img src="modern_architectures_figs/3-5_L2_cache.png">
                </section>

                <section><h3>Cache memory</h3>
                <p>Consider same example again, 8-core 3 GHz CPU with 384 GFlop/s peak performance, main memory bandwidth of 51.2 GB/s.</p>
                <p>Now add a (shared) cache of 512 KB, but assume access happens within a clock cycle (operates at register-speed).</p>
                <p>Consider a matrix product operation, where all matrices are $n \times n$ with $n = 128$:</p>
                <pre><code>for (int i = 0; i &lt n; i++)
    for (int j = 0; j &lt n; j++) {
        double dotp = 0.0;
        for (int k = 0; k &lt n; k++)
            dotp += U[i][k] * V[k][j];
        W[i][j] = dotp;
    }</code></pre>
                </section>

                <section><h4>Cache memory</h4>
                <p>Matrix dimensions are small: $128 \times 128 \times 8$ B $= 128$ KB per matrix (double precision), so all three ($3 \times 128 = 384$ KB) fit into cache.</p>
                <p>For each of the $n^2$ values in $W$ we calculate a scalar product with $2\cdot n$ operations, so we have $2\cdot n^3 = 2\cdot 128^3 = 2^{22}$ Flops.</p>
                <p><ul><li><b>Computation</b>: $t_{comp} = 2^{22} \text{Flop}/384 \text{GFlop/s} = 10.4 \mu\text{s}$</li>
                       <li><b>Data transfer</b>: $t_{mem} = 384 \text{KB}/51.2 \text{GB/s} = 7.5 \mu\text{s}$ (to load all matrices into cache at start)</li></ul></p>
                </section>
                
                <section><h4>Cache memory</h4>
                <p>Due to usage of cache, we can re-use each element of the input matrices during the $n$ different scalar product calculations.</p>
                <p>Compute time is now longer than data transfer: matrix multiplication is <i>compute-bound</i> (or CPU-bound).</p>
                <p>Assuming no overlap of computation and transfer, $t_{exec} \geq t_{comp} + t_{mem} = 10.4 \mu\text{s} + 7.5 \mu\text{s} = 17.9 \mu\text{s}$.</p>
                <p>This corresponds to $2^{22} \text{Flop}/17.9 \mu\text{s} = 223$ GFlop/s, 60% of peak performance.</p>
                </section>

                <section><h3>Cache algorithms</h3>
                <p>Previous example assumed data fits into cache. This is usually <b>not</b> true.</p>
                <p>We need a set of cache replacement policies (<i>cache algorithms</i>) to determine which data is stored in cache during execution.</p>
                <p>Cache algorithms address the following questions:</p>
                <p><ul><li>Which data do we load from main memory and where in cache do we store it?</li>
                       <li>If cache is already full, which data do we evict?</li></ul></p>
                </section>

                <section><h4>Cache algorithms</h4>
                <p>When CPU requests a data item from memory, the cache is checked first to see if data is already there.</p>
                <p>If so: <b>cache hit</b>. Otherwise: <b>cache miss</b>. Cache algorithms try to maximise the <b>hit ratio</b>.</p>
                <p>The design of cache algorithms is guided by two principles:</p>
                <p><ul><li>Spatial locality: contiguous memory locations.</li>
                       <li>Temporal locality: cache data that was recently used is maintained in cache.</li></ul></p>
                </section>

                <section><h4>Cache algorithms: spatial locality</h4>
                <p>Consider the following code to determine the maximum value of an array $A$ of size $n$ (where the elements of $A$ are stored contiguously in memory).</p>
                <pre><code>for (int i = 0; i &lt n; i++)
    maximum = max(A[i], maximum);</code></pre>
                <p><ul><li>Assume cache is initially empty. In first iteration $A[0]$ is requested from memory: cache miss.</li>
                <li>Rather than one value, a <b>cache line</b> is loaded with values from neighbouring memory addresses.</li>
                <li>Assuming a $64$ B cache line and double-precision, this would load the eight values $A[0]$ to $A[7]$ into cache.</li>
                <li>The next seven iterations result in cache hits. The request for $A[8]$ is a cache miss, etc.</li></ul></p>
                <p>The hit ratio is 87.5% in this example.</p>
                </section>

                <section><h4>Cache algorithms: temporal locality</h4>
                <p>Cache is organised into blocks (cache lines) of fixed size (e.g. $64$ B).</p>
                <p>Cache mapping determines in which block a particular entry of main memory is stored:</p>
                <p><ul><li><i>Direct-mapped cache</i>: each block from memory is stored in exactly one cache line (typically low hit ratio).</li>
                       <li><i>Two-way associative cache</i>: each block from memory is stored in one of two possible cache lines. A strategy to decide which is <i>least-recently used</i> (LRU): we evict the least recently accessed entry (improves hit ratio substantially).</li>
                       <li><i>$n$-way associative cache</i>: usually with $n = 2$, $4$ or $8$. Fully associative caches are usually too expensive.</li></ul></p>
                </section>

                <section><h4>Cache algorithms: temporal locality</h4>
                <img src="modern_architectures_figs/3-3_cache_types.png">
                <p>(A) direct-mapped, (B) two-way associative</p>
                </section>

                <section><h3>Optimising cache access</h3>
                <p><ul><li>Consider matrix multiplication again, $A \cdot B = C$, with dimensions $m \times l$, $l \times n$ and $m \times n$.</li>
                <li>The matrices are stored in linear arrays in row-major order. The following C code implements the matrix multiplication in two ways.</li></ul></p>
                <pre><code>#include &ltstdio.h&gt
#include &ltstdint.h&gt
#include &ltstdlib.h&gt
#include &lttime.h&gt

int main() {

    const uint64_t m = 1 &lt&lt 10;
    const uint64_t n = 1 &lt&lt 10;
    const uint64_t l = 1 &lt&lt 10;

    clock_t t;

    t = clock();
    float* A = malloc(sizeof(float)*m*l);
    float* B = malloc(sizeof(float)*l*n);
    float* Bt = malloc(sizeof(float)*n*l);
    float* C = malloc(sizeof(float)*m*n);

    for (uint64_t i = 0; i &lt m*l; i ++){
      A[i] = 0.0;
      B[i] = 0.0;
      Bt[i] = 0.0;
      C[i] = 0.0;
    }

    t = clock() - t;
    
    double time_taken = ((double)t)/CLOCKS_PER_SEC;
    printf("init %f\n",time_taken);

    t = clock();
    for (uint64_t i = 0; i &lt m; i++) {
      for (uint64_t j = 0; j &lt n; j++) {
          float accum = 0;
          for (uint64_t k = 0; k &lt l; k++) {
              accum += A[i*l+k]*B[k*n+j];
          }
          C[i*n+j] = accum;
       }
    }
    t = clock() - t;

    time_taken = ((double)t)/CLOCKS_PER_SEC;
    printf("naive %f\n",time_taken);

    t = clock();
    for (uint64_t k = 0; k &lt l; k++) {
        for (uint64_t j = 0; j &lt n; j++) {   
            Bt[j*l+k] = B[k*n+j];
        }
    }
    t = clock() - t;

    time_taken = ((double)t)/CLOCKS_PER_SEC;
    printf("transpose %f\n",time_taken);

    t = clock();
    for (uint64_t i = 0; i &lt m; i++) {
        for (uint64_t j = 0; j &lt n; j++) {
            float accum = 0;
            for (uint64_t k = 0; k &lt l; k++) {
                accum += A[i*l+k]*Bt[j*l+k];
            }
            C[i*n+j] = accum;
        }
    }
    t = clock() - t;

    time_taken = ((double)t)/CLOCKS_PER_SEC;
    printf("transpose_mult %f\n",time_taken);

    return 0;
}</code></pre>
                </section>

                <section><h4>Optimising cache access</h4>
                <p>The first loop uses a "naive" method, where access to matrix $B$ is not contiguous.</p>
                <p>Values in row $k$ and $k+1$ are $l \times \text{sizeof}(\text{float})$ byte-addresses apart in memory, and will not be in the same cache line.</p>
                <p>If $l$ is large enough the cache will be cleaned before the cache line is accessed, leading to a low hit ratio.</p>
                <p>The last loop uses the transpose of $B$, allowing memory contiguity (spatial locality) and more cache hits.</p>
                </section>

                <section><h4>Optimising cache access</h4>
                <img src="modern_architectures_figs/3-4_array_multiplication_cache_miss.png">
                </section>

                <section><h3>Cache coherence</h3>
                <p>What about writing to memory? Only modifying cached value leads to <i>inconsistency</i>.</p>
                <p>Two policies are used to ensure <i>coherence</i> between cache and main memory:</p>
                <p><ul><li><b>Write-through</b>: we write to the main memory address associate to the cache line. Slow if there are many writes.</li>
                       <li><b>Write-back</b>: the cache line is modified and marked as <i>dirty</i>. When evicting a dirty cache line the data is written to main memory.</li></ul></p>
                <div class="container">
                <div class="col">
                <p>The situation is more complicated in a multi-core CPU with multi-level cache: private low-level L1 cache, shared high-level L2 cache</p>
                </div>
                <div class="col">
                <img src="modern_architectures_figs/3-5_L2_cache.png" width=300>
                </div>
                </div>
                <p>Various protocols have been developed to implement cache coherence in parallel systems, e.g. <i>MESI</i>.</p>
                </section>

                <section><h4>Cache coherence</h4>
                <img src="modern_architectures_figs/3-6_cache_incoherence.png" width=600>
                </section>

                <section><h3>False sharing</h3>
                <p>Imagine a multi-core system operating on distinct data items (fitting a single cache line) stored in the same region of main memory.</p>
                <p>When one core writes to its own cache line, this invalidates the cache line for all other cores.</p>
                <p>These cores then need to retrieve the data from main memory again, even though they have not modified anything.</p>
                <p>This is <i>false sharing</i> or <i>cache line ping-pong</i> and it severely degrades performance.</p>
                </section>

                <section><h3>Levels of parallelism</h3>
                <p>Flynn's taxonomy:</p>
                <p><ul><li><b>SISD (Single Instruction, Single Data)</b>: traditional von Neumann architecture.</li>
                       <li><b>SIMD (Single Instruction, Multiple Data)</b>: same operation performed on distinct data items in parallel.</li>
                       <li><b>MIMD (Multiple Instruction, Multiple Data)</b>: multiple processing units execute different instructions on different data.</li>
                       <li><b>MISD (Multiple Instruction, Single Data)</b>: multiple processing units execute different instructions on a single data stream. Not common.</li></ul></p>
                </section>

                <section><h4>Levels of parallelism</h4>
                <img src="modern_architectures_figs/3-7_flynn_taxonomy.png" width=600>
                </section>

                <section><h4>Levels of parallelism</h4>
                <p>Modern CPUs and GPUs exploit different levels of parallelism.</p>
                <p><ul><li><b>Multiple cores</b>: MIMD parallelism.</li>
                       <li><b>Vector units in each core</b>: SIMD parallelism.</li>
                       <li><b>Instruction-level parallelism</b>: pipelining, superscalar execution (Not covered here).</li></ul></p>
                </section>

                <section><h3>SIMD concept</h3>
                <p>The same instruction is executed on multiple data elements.</p>
                <p>Example with a sequential loop:</p>
                <pre><code>for (i = 0; i &lt n; i++)
    w[i] = u[i] - v[i];</code></pre>
                <p>We want to map this loop to a SIMD machine.</p>
                <p>Each iteration is independent so this is easily parallelised.</p>
                <img src="modern_architectures_figs/3-8_simd_parallel_subtract.png" width=400>
                </section>

                <section><h4>SIMD concept</h4>
                <p>Not easily parallelised example, because of a conditional:</p>
                <pre><code>for (i = 0; i &lt n; i++)
    if (u[i] &gt 0)
        w[i] = u[i] - v[i];
    else
        w[i] = u[i] + v[i];</code></pre>
                <p>This can be SIMD parallelised if we allow a processing unit to <b>idle</b>.</p>
                </section>

                <section><h4>SIMD concept</h4>
                <p>We can have a parallel implementation of the loop with conditional using three stages:</p>
                <img src="modern_architectures_figs/3-9_simd_conditional_add.png" width=400>
                <p><ol><li>Each processing unit compares its register $U$ value to 0 and sets a "larger" flag.</li>
                       <li>$U-V$ is executed by all units but the result is only stored whenever the flag is set.</li>
                       <li>$U+V$ is executed by all units but the result is only stored whenever the flag is not set.</li></ol></p>
                <p>The conditional reduces the SIMD efficiency of the loop by 50%.</p>
                </section>

                <section><h4>SIMD concept</h4>
                <p>Modern CPUs typically have a vector unit which can be explicitly programmed to achieve SIMD parallelisation (next section).</p>
                <p>On CUDA-enabled GPUs all threads in a <i>warp</i> operate in (approximately) SIMD fashion (next semester).</p>
                </section>

                <section><h3>Vectorisation on common microprocessors</h3>
                <p>Support for SIMD operations is now common on most CPUs, with explicit instruction sets:</p>
                <p><b>AVX</b>: 256-bit vector register length.</p>
                <p><b>AVX-512</b>: 512-bit vector register length.</p>
                </section>

                <section><h4>Vectorisation: SIMD intrinsics</h4>
                <p>Intrinsics consist of assembly-coded functions and data-type definitions that can be used in C and C++ source code.</p>
                <p>Example: addition of two 256-bit AVX registers:</p>
                <pre><code>__m256 a,b,c;            // Declare AVX registers
...                      // Initialise a and b
c = __m256_add_ps(a,b);  // c[0:8] = a[0:8] + b[0:8]</code></pre>
                <p>A variable of type "__m256" represents a 256-bit long AVX register holding $8$ $32$-bit floating point values.</p>
                <p>The AVX intrinsic "__m256_add_ps" performs an addition in 8-way SIMD fashion.</p>
                </section>

                <section><h4>Vectorisation: SIMD intrinsics</h4>
                <img src="modern_architectures_figs/3-10_avx_addition.png">
                </section>

                <section><h4>Example code SIMD intrinsics</h4>
                <p><pre><code>#include &ltstdint.h&gt
#include &ltstdio.h&gt
#include &ltimmintrin.h&gt
#include &lttime.h&gt

void plain_vecadd(float* A, float* B, float* C, uint64_t N){

    for (uint64_t i = 0; i &lt N; i++){
        C[i] = A[i] + B[i];
    }

}

void avx2_vecadd(float* A, float* B, float* C, uint64_t N){

    __m256 X;
    for (uint64_t i = 0; i &lt N; i += 8){
        __m256 AV = _mm256_load_ps(A+i);
        __m256 BV = _mm256_load_ps(B+i);
        X = _mm256_add_ps(AV,BV);
        _mm256_store_ps(C+i, X);
    }

}

void populate_vectors(float* A, float* B, uint64_t N){

    for (uint64_t i = 0; i &lt N; i++){
        A[i] = i*1.0;
        B[i] = i*2.0;
    }

}

int main(){

    clock_t begin, end;
    double time_spent;

    uint64_t N = 1 &lt&lt 20;

    printf("Vector elements: %lu\n",N);

    uint64_t error_flag = 0;

    float* A = _mm_malloc(sizeof(float)*N, 32); //align on 32-byte boundaries for AVX registers
    float* B = _mm_malloc(sizeof(float)*N, 32);
    float* C = _mm_malloc(sizeof(float)*N, 32);
    float check;

    populate_vectors(A, B, N);

    begin = clock();
    plain_vecadd(A, B, C, N);
    end = clock();
    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;
    printf("Serial add: %f\n",time_spent);

    begin = clock();
    avx2_vecadd(A, B, C, N);
    end = clock();
    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;
    printf("Vector add: %f\n",time_spent);

    for (uint64_t i = 0; i &lt N; i++){
        check = i*3.0 - C[i];
        if (check != 0.0){
           error_flag = 1;
        }
    }

    if (error_flag == 1){
        printf("Error in sum.\n");
    }
}</code></pre></p>
                <p><code>gcc -march=skylake vector_addition.c -o vector_addition.x</code></p>
                </section>

                <section><h4>Example code SIMD intrinsics</h4>
                <p>Listing 3.2 uses AVX2 (extension of AVX) intrinsics for matrix multiplication $A \times B = C$.</p>
                <img src="modern_architectures_figs/3-11_add_intrinsic.png">
                <p>Illustration of _mm256_fmadd_ps(AV,BV,X) intrinsic (used in Listing 3.2)</p>
                </section>

                <section><h4>Example code SIMD intrinsics</h4>
                <pre><code>#include &ltcstdint&gt // uint32_t
#include &ltiostream&gt // std::cout
#include &ltimmintrin.h&gt // AVX intrinsics

void plain_tmm(float* A,
               float* B,
               float* C,
               uint64_t M,
               uint64_t L,
               uint64_t N) {

    for (uint64_t i = 0; i &lt M; i++)
        for (uint64_t j = 0; j &lt N; j++) {
            float accum = float(0);
            for (uint64_t k = 0; k &lt L; k++)
                accum += A[i*L+k]*B[j*L+k];
            C[i*N+j] = accum;
        }
}

void avx2_tmm(float* A,
              float* B,
              float* C,
              uint64_t M,
              uint64_t L,
              uint64_t N) {

    for (uint64_t i = 0; i &lt M; i++)
        for (uint64_t j = 0; j &lt N; j++) {

            __m256 X = _mm256_setzero_ps();
            for (uint64_t k = 0; k &lt L; k += 8) {
                const __m256 AV = _mm256_load_ps(A+i*L+k);
                const __m256 BV = _mm256_load_ps(B+j*L+k);
                X = _mm256_fmadd_ps(AV,BV,X);
            }

            C[i*N+j] = hsum_avx(X);
        }
}</code></pre>
                <p>The implementation of <code>hsum_avx</code> is left as an exercise.</p>
                </section>

                <section><h3>AoS and SoA</h3>
                <p>How we organise the data has an impact on the efficiency of a SIMD parallelisation.</p>
                <p><ul><li><b>AoS (Array of Structures)</b> stores data consecutively in an array.</li>
                       <li><b>SoA (Structure of Arrays)</b> uses one array per dimension of the data (multiple arrays).</li></ul></p>
                </section>

                <section><h4>AoS/SoA case study: 3D vectors</h4>
                <p>Consider $n$ real-valued 3D vectors.</p>
                <p>AoS is a single array of $3n$ elements. SoA would be 3 arrays of $n$ elements.</p>
                <img src="modern_architectures_figs/3-12_aos_and_soa.png">
                <p><ul><li>Calculating norm with a SIMD implementation is more efficient using SoA</li>
                <li>We can transpose from one to the other if necessary</li></ul></p>
                </section>

                <section><h4>AoS/SoA case study: 3D vectors</h4>
                <p>AoS definition:</p>
                <p><pre><code>float* xyz = malloc(sizeof(float)*3*n);</code></pre></p>
                <p>SoA definition:</p>
                <p><pre><code>float* x = malloc(sizeof(float)*n);
float* y = malloc(sizeof(float)*n);
float* z = malloc(sizeof(float)*n);</code></pre></p>
                </section>

                <section><h4>Normalising vectors</h4>
                <p>$\hat{v}_i = \frac{v_i}{||v_i||} = \left( \frac{x_i}{\rho_i}, \frac{y_i}{\rho_i}, \frac{z_i}{\rho_i} \right)$ where $\rho_i = \sqrt{x_i^2 + y_i^2 + z_i^2}$</p>
                <p>This norm can be calculated for AoS data in sequential code in the following way:</p>
                <p><pre><code>void plain_aos_norm(float* xyz, uint64_t length) {

    for (uint64_t i = 0; i &lt 3*length; i += 3) {
        const float x = xyz[i+0];
        const float y = xyz[i+1];
        const float z = xyz[i+2];
        float irho = 1.0f/sqrtf(x*x+y*y+z*z);

        xyz[i+0] *= irho;
        xyz[i+1] *= irho;
        xyz[i+2] *= irho;
    }
}</code></pre></p>
                </section>

                <section><h4>Normalising vectors</h4>
                <p>Vectorisation of this normalisation operation (using AoS) is inefficient because:</p>
                <p><ul><li>Vector registers would not be fully occupied: 128-bit register with single precision floats, single vector would occupy $3$ of $4$ available vector lanes.</li>
                       <li>Summing squares for the <code>irho</code> calculation requires operations between horizontal lanes resulting in only a single value for the inverse square root calculation.</li>
                       <li>Scaling to longer vector registers becomes increasingly inefficient.</li></ul></p>
                </section>

                <section><h4>Normalising vectors</h4>
                <p>Vectorisation is much more efficient with SoA:</p>
                <p><pre><code>void avx_soa_norm(float* x, float* y, float* z, uint64_t length) {

    for (uint64_t i = 0; i &lt length; i += 8) {

        __m256 X = _mm256_load_ps(x+i);
        __m256 Y = _mm256_load_ps(y+i);
        __m256 Z = _mm256_load_ps(z+i);

        __m256 R = _mm256_fmadd_ps(X, X,
                   _mm256_fmadd_ps(Y, Y,
                   _mm256_mul_ps  (Z, Z)));

        R = _mm256_rsqrt_ps(R);

        _mm256_store_ps(x+i, _mm256_mul_ps(X, R));
        _mm256_store_ps(y+i, _mm256_mul_ps(Y, R));
        _mm256_store_ps(z+i, _mm256_mul_ps(Z, R));
    }
}</code></pre></p>
                </section>

                <section><h4>Shuffling between AoS/SoA</h4>
                <p>We can use vector registers to reorganise AoS to SoA and back again.</p>
                <p><ol><li>Transpose 8 consecutive 3D vectors from AoS to SoA using 256-bit registers.</li>
                       <li>Perform vectorised SIMD computation using SoA.</li>
                       <li>Transpose result from SoA to AoS.</li></ol></p>
                </section>

                <section><h4>Shuffling between AoS/SoA</h4>
                <img src="modern_architectures_figs/3-13_aos2soa.png">
                </section>

                <section><h4>Shuffling code: new instrinsics</h4>
                <p><code>__m256 _mm256_shuffle_ps(__m256 m1, __m256 m2, const int sel)</code></p>
                <p><code>sel</code> is a set of 4 2-bit values (four numbers between 0 and 3) used to select from the elements of <code>m1</code> and <code>m2</code> and then placed in output vector.</p>
                <p>Example: shuffle operation 2 in diagram is implemented by:</p>
                <p><pre><code>YZ = _mm256_shuffle_ps(M03, M14, _MM_SHUFFLE(1,0,2,1));</code></pre></p>
                <p>The elements 1 and 2 from the first four elements of M03 are added to the new vector, then the elements 0 and 1 from the first four elements of M14 are added.</p>
                </section>

                <section><h4>Shuffling code: new intrinsics</h4>
                <p><code>__m256 _mm256_castps128_ps256(__m128 a)</code></p>
                <p>Typecasts a 128-bit vector into a 256-bit vector. Lower half of output vector contains values from source, upper half undefined.</p>
                <p><code>__m256 _mm256_insertf128_ps(__m256 a, __m128 b, int offset)</code></p>
                <p>Inserts a 128-bit vector into a 256-bit vector at a position given by <code>offset</code>.</p>
                <p>For example, to insert two 128-bit registers <code>M[0]</code> and <code>M[3]</code> into a single 256-bit register <code>M03</code>:</p>
                <p><pre><code>M03 = _mm256_castps128_ps256(M[0]);
M03 = _mm256_insertf128_ps(M03, M[3], 1);</code></pre></p>
                </section>

                <section><h4>Shuffling code</h4>
                <p><pre><code>#include &ltgsl/gsl_rng.h&gt
#include &ltstdint.h&gt
#include &ltstdio.h&gt
#include &ltimmintrin.h&gt

void aos_init(float* xyz, uint64_t length){
  const gsl_rng_type* T;
  gsl_rng* r;

  gsl_rng_env_setup();
  T = gsl_rng_default;
  r = gsl_rng_alloc(T);

  for (uint64_t i = 0; i &lt 3*length; i++){
    xyz[i] = gsl_rng_uniform(r);
  }
}

void avx_aos_norm(float* xyz, uint64_t length){
  for (uint64_t i = 0; i &lt 3*length; i += 3*8){
    /////////////////////////////////////////////
    // AoS2SoA: XYZXYZXYZ --&gt XXX YYY ZZZ
    /////////////////////////////////////////////
  
    // registers: note M is an SSE pointer (length 4)
    __m128* M = (__m128*) (xyz+i);
    __m256 M03, M14, M25;

    // load lower halves
    M03 = _mm256_castps128_ps256(M[0]);
    M14 = _mm256_castps128_ps256(M[1]);
    M25 = _mm256_castps128_ps256(M[2]);

    // load upper halves
    M03 = _mm256_insertf128_ps(M03, M[3], 1);
    M14 = _mm256_insertf128_ps(M14, M[4], 1);
    M25 = _mm256_insertf128_ps(M25, M[5], 1);

    // shuffle
    __m256 XY = _mm256_shuffle_ps(M14, M25, _MM_SHUFFLE(2,1,3,2));
    __m256 YZ = _mm256_shuffle_ps(M03, M14, _MM_SHUFFLE(1,0,2,1));
    __m256 X = _mm256_shuffle_ps(M03, XY, _MM_SHUFFLE(2,0,3,0));
    __m256 Y = _mm256_shuffle_ps(YZ, XY, _MM_SHUFFLE(3,1,2,0));
    __m256 Z = _mm256_shuffle_ps(YZ, M25, _MM_SHUFFLE(3,0,3,1));

    //////////////////
    // SoA computation
    //////////////////
    
    // R &lt- X*X+Y*Y+Z*Z
    __m256 R = _mm256_fmadd_ps(X, X,
               _mm256_fmadd_ps(Y, Y,
               _mm256_mul_ps  (Z, Z)));

    // R &lt- 1/sqrt(R)
    R = _mm256_rsqrt_ps(R);

    // normalise vectors
    X = _mm256_mul_ps(X, R);
    Y = _mm256_mul_ps(Y, R);
    Z = _mm256_mul_ps(Z, R);

    /////////////////////////////////////
    // SoA2AoS: XXX YYY ZZZ --&gt XYZXYZXYZ
    /////////////////////////////////////

    // shuffle
    __m256 RXY = _mm256_shuffle_ps(X, Y, _MM_SHUFFLE(2,0,2,0));
    __m256 RYZ = _mm256_shuffle_ps(Y, Z, _MM_SHUFFLE(3,1,3,1));
    __m256 RZX = _mm256_shuffle_ps(Z, X, _MM_SHUFFLE(3,1,2,0));
    __m256 R03 = _mm256_shuffle_ps(RXY, RZX, _MM_SHUFFLE(2,0,2,0));
    __m256 R14 = _mm256_shuffle_ps(RYZ, RXY, _MM_SHUFFLE(3,1,2,0));
    __m256 R25 = _mm256_shuffle_ps(RZX, RYZ, _MM_SHUFFLE(3,1,3,1));

    // store in AoS
    M[0] = _mm256_castps256_ps128(R03);
    M[1] = _mm256_castps256_ps128(R14);
    M[2] = _mm256_castps256_ps128(R25);
    M[3] = _mm256_extractf128_ps(R03, 1);
    M[4] = _mm256_extractf128_ps(R14, 1);
    M[5] = _mm256_extractf128_ps(R25, 1);
  }
}

int main(){
  const uint64_t num_vectors = 1UL &lt&lt 28;
  const uint64_t num_bytes = 3*num_vectors*sizeof(float);

  float* xyz = (float*)_mm_malloc(num_bytes, 32);

  aos_init(xyz, num_vectors);

  avx_aos_norm(xyz, num_vectors);

  _mm_free(xyz);
}</code></pre></p>
                <p><code>gcc -march=skylake vector_norms.c -o vector_norms.x -lgsl -lcblas</code></p>
                <p>Example in book: speedup of 2.2 compared to AoS norm</p>
                </section>

                <section><h3>AVX naming conventions (according to Intel website)</h3>
                <table><tr><td>__m256</td><td>256-bit as eight single-precision floating-point values (Intel chip register width)</td></tr>
                       <tr><td>__m256d</td><td>256-bit as four double-precision floating-point values</td></tr>
                       <tr><td>__m256i</td><td>256-bit as integers (bytes, words, etc.)</td></tr>
                       <tr><td>__m128</td><td>128-bit single-precision floating-point (32 bits each)</td></tr>
                       <tr><td>__m128d</td><td>128-bit single-precision floating-point (64 bits each)</td></tr></table>
                <p>Functions: _mm256_&ltoperator&gt_&ltsuffix&gt(data_type param1, data_type param2, data_type param3)</p>
                <p>Operators are add, sub, etc. Suffix is the type of data to act on</p>
                </section>

			</div>
		</div>

<script>

require(
    {
      // it makes sense to wait a little bit when you are loading
      // reveal from a cdn in a slow connection environment
      waitSeconds: 15
    },
    [
      "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/lib/js/head.min.js",
      "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/js/reveal.js"
    ],

    function(head, Reveal){

        // Full list of configuration options available here: https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,

            transition: "slide",

            // Optional libraries used to extend on reveal.js
            dependencies: [
                { src: "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/plugin/highlight/highlight.js" }
            ]
        });

        var update = function(event){
          if(MathJax.Hub.getAllJax(Reveal.getCurrentSlide())){
            MathJax.Hub.Rerender(Reveal.getCurrentSlide());
          }
        };

        Reveal.addEventListener('slidechanged', update);

        function setScrollingSlide() {
            var scroll = false
            if (scroll === true) {
              var h = $('.reveal').height() * 0.95;
              $('section.present').find('section')
                .filter(function() {
                  return $(this).height() > h;
                })
                .css('height', 'calc(95vh)')
                .css('overflow-y', 'scroll')
                .css('margin-top', '20px');
            }
        }

        // check and set the scrolling slide every time the slide change
        Reveal.addEventListener('slidechanged', setScrollingSlide);

    }

);
</script>
	</body>
</html>
